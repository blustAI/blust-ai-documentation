"use strict";(self.webpackChunkblust_ai_documentation=self.webpackChunkblust_ai_documentation||[]).push([[180],{1200:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var t=i(7624),s=i(2172);const a={sidebar_position:10,sidebar_label:"Available models",title:"Available models"},o=void 0,r={id:"creating-ai-tools/available_models",title:"Available models",description:"Text generation models",source:"@site/docs/creating-ai-tools/available_models.md",sourceDirName:"creating-ai-tools",slug:"/creating-ai-tools/available_models",permalink:"/docs/creating-ai-tools/available_models",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,sidebar_label:"Available models",title:"Available models"},sidebar:"tutorialSidebar",previous:{title:"Configuration",permalink:"/docs/creating-ai-tools/configuration"},next:{title:"Integrating an existing AI Tool",permalink:"/docs/category/integrating-an-existing-ai-tool"}},l={},d=[{value:"Text generation models",id:"text-generation-models",level:4},{value:"Image generation models",id:"image-generation-models",level:4},{value:"Voice to text models",id:"voice-to-text-models",level:4},{value:"Text to voice models",id:"text-to-voice-models",level:4},{value:"Image recognition models",id:"image-recognition-models",level:4},{value:"Multimodal models",id:"multimodal-models",level:4},{value:"Assistants",id:"assistants",level:4}];function c(e){const n={em:"em",h4:"h4",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.M)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h4,{id:"text-generation-models",children:"Text generation models"}),"\n",(0,t.jsx)(n.p,{children:"Models in this category specialize in generating coherent and contextually relevant text across a wide variety of topics and formats. They are designed to understand and produce written language, making them ideal for applications ranging from content creation to conversation simulation."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-3.5 Turbo"}),": powered by OpenAI, is a faster variant of the original  GPT-3.5 model, designed for quick text generation with maintained quality across diverse domains."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4"}),": The fourth iteration of the Generative Pre-trained Transformer, known for its deep comprehension and versatile text generation across numerous contexts."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4 Turbo"}),": A turbocharged variant of GPT-4 designed for high-speed text generation, combining GPT-4's advanced understanding with improved processing speed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gemini Pro"}),": Gemini Pro is a scalable, general-purpose AI model developed by Google, designed to handle a variety of information. It stands out for its capability to outperform other models of similar size on research benchmarks, offering features like function calling, embeddings, semantic retrieval, custom knowledge grounding, and chat functionality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PaLM 2"})," ",(0,t.jsx)(n.em,{children:"(legacy)"}),":  Google's advanced large language model that contributed to generative AI capabilities across various Google technologies."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Llama-2"}),": A next-generation large language model from Meta, focused on generative AI applications."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Claude v2.1"}),": Developed by Anthropic, this advanced language model features a 200K token context window, reduced hallucination rates, and heightened accuracy. It's adept at processing extensive content for a range of applications, including business and legal document analysis\u200b"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mistral 7B instruct"}),": A 7.3 billion parameter language model by Mistral AI, fine-tuned for conversational and question-answering tasks. Designed for real-time applications, it showcases adaptability and efficiency across a variety of tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"image-generation-models",children:"Image generation models"}),"\n",(0,t.jsx)(n.p,{children:"These models are capable of creating detailed and diverse visual content from textual descriptions. They combine elements of creativity and artificial intelligence to produce images that match specified prompts, useful in design, art, and visual content generation."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DALL-E 2"}),": An AI model by OpenAI capable of generating complex images from textual descriptions, known for its creativity and high fidelity outputs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DALL-E 3"}),": The successor to DALL-E 2, improving upon its predecessor with enhanced image quality and generation capabilities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenJorney"}),": A versatile image generation model designed to create detailed and diverse visual content based on textual prompts."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stable Diffusion 2.1"}),": A model that excels in generating high-quality images from textual descriptions, known for its speed and versatility."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"voice-to-text-models",children:"Voice to text models"}),"\n",(0,t.jsx)(n.p,{children:"Voice to text models are engineered to accurately transcribe spoken language into written text. They are adept at understanding various languages, accents, and dialects, making them essential for dictation, accessibility features, and voice-controlled applications."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper"}),": An advanced voice recognition model powered by OpenAI, designed to accurately transcribe spoken language into text, excelling in multiple languages and dialects."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"text-to-voice-models",children:"Text to voice models"}),"\n",(0,t.jsx)(n.p,{children:"Models in this category convert written text into natural-sounding spoken language. They are utilized in applications that require speech synthesis, such as virtual assistants, audiobooks, and spoken content for the visually impaired."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TTS"}),": An OpenAI API that generates high-quality spoken audio from text, featuring six preset voices for real-time use and enhanced quality. Optimized for diverse applications, it offers streaming capabilities and supports inputs up to 4096 characters."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"image-recognition-models",children:"Image recognition models"}),"\n",(0,t.jsx)(n.p,{children:"These models analyze and interpret visual information from images or video streams. They can identify objects, scenes, and activities, making them useful for tasks such as image captioning, surveillance, and automated content categorization."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT2 image captioning"}),": A basic AI model designed for generating textual descriptions of uploaded images, leveraging GPT-2's capabilities for visual recognition and interpretation."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"multimodal-models",children:"Multimodal models"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal models can process and generate content that spans multiple types of input and output, such as text and images. Their versatility allows them to understand and create rich multimedia content, serving applications that require a combination of visual and textual comprehension."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4 Turbo Vision"}),": Combines the text generation prowess of GPT-4 Turbo with visual understanding, enabling it to process and generate content based on both text and images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gemini Pro Vision"}),"*: A Google-developed multimodal AI model capable of processing text, images, video, audio, and code. Optimized for a wide range of tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"assistants",children:"Assistants"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4 Turbo Assistant"}),": A specialized version of GPT-4 Turbo designed to act as a virtual assistant, providing high-speed responses to queries across a wide range of topics."]}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,s.M)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},2172:(e,n,i)=>{i.d(n,{I:()=>r,M:()=>o});var t=i(1504);const s={},a=t.createContext(s);function o(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);